{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"source":"<a href=\"https://www.kaggle.com/code/mvsaikumar/customer-churn-prediction-log-reg?scriptVersionId=108417954\" target=\"_blank\"><img align=\"left\" alt=\"Kaggle\" title=\"Open in Kaggle\" src=\"https://kaggle.com/static/images/open-in-kaggle.svg\"></a>","metadata":{},"cell_type":"markdown","outputs":[],"execution_count":0},{"cell_type":"markdown","source":"## Customer Churn Prediction using Logisitic Regression","metadata":{}},{"cell_type":"markdown","source":"## Data Dictionary\n\nThere are multiple variables in the dataset which can be cleanly divided in 3 categories:\n\n### Demographic information about customers\n\n<b>customer_id</b> - Customer id\n\n<b>vintage</b> - Vintage of the customer with the bank in number of days\n\n<b>age</b> - Age of customer\n\n<b>gender</b> - Gender of customer\n\n<b>dependents</b> - Number of dependents\n\n<b>occupation</b> - Occupation of the customer \n\n<b>city</b> - City of customer (anonymised)\n\n\n### Customer Bank Relationship\n\n\n<b>customer_nw_category</b> - Net worth of customer (3:Low 2:Medium 1:High)\n\n<b>branch_code</b> - Branch Code for customer account\n\n<b>days_since_last_transaction</b> - No of Days Since Last Credit in Last 1 year\n\n\n### Transactional Information\n\n<b>current_balance</b> - Balance as of today\n\n<b>previous_month_end_balance</b> - End of Month Balance of previous month\n\n\n<b>average_monthly_balance_prevQ</b> - Average monthly balances (AMB) in Previous Quarter\n\n<b>average_monthly_balance_prevQ2</b> - Average monthly balances (AMB) in previous to previous quarter\n\n<b>current_month_credit</b> - Total Credit Amount current month\n\n<b>previous_month_credit</b> - Total Credit Amount previous month\n\n<b>current_month_debit</b> - Total Debit Amount current month\n\n<b>previous_month_debit</b> - Total Debit Amount previous month\n\n<b>current_month_balance</b> - Average Balance of current month\n\n<b>previous_month_balance</b> - Average Balance of previous month\n\n<b>churn</b> - Average balance of customer falls below minimum balance in the next quarter (1/0)","metadata":{}},{"cell_type":"markdown","source":"## Churn Prediction using Logisitic Regression\n\nNow, that we understand the dataset in detail. It is time to build a logistic regression model to predict the churn. I have included the data dictionary again here for reference.\n\n* Load Data & Packages for model building & preprocessing\n* Preprocessing & Missing value imputation\n* Select features on the basis of EDA Conclusions & build baseline model\n* Decide Evaluation Metric on the basis of business problem\n* Build model using all features & compare with baseline\n* Use Reverse Feature Elimination to find the top features and build model using the top 10 features & compare","metadata":{}},{"cell_type":"markdown","source":"### Loading Packages","metadata":{}},{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.model_selection import KFold, StratifiedKFold, train_test_split\nfrom sklearn.metrics import roc_auc_score, accuracy_score, confusion_matrix, roc_curve, precision_score, recall_score, precision_recall_curve\n\nimport warnings\nwarnings.simplefilter(action='ignore', category=FutureWarning)\nwarnings.simplefilter(action='ignore', category=UserWarning)","metadata":{"execution":{"iopub.status.busy":"2022-10-18T06:45:52.766957Z","iopub.execute_input":"2022-10-18T06:45:52.767842Z","iopub.status.idle":"2022-10-18T06:45:54.384761Z","shell.execute_reply.started":"2022-10-18T06:45:52.767691Z","shell.execute_reply":"2022-10-18T06:45:54.383441Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Loading Data","metadata":{}},{"cell_type":"code","source":"df = pd.read_csv('../input/customer-churn-prediction/churn_prediction.csv')","metadata":{"execution":{"iopub.status.busy":"2022-10-18T06:45:54.386795Z","iopub.execute_input":"2022-10-18T06:45:54.387246Z","iopub.status.idle":"2022-10-18T06:45:54.587384Z","shell.execute_reply.started":"2022-10-18T06:45:54.387216Z","shell.execute_reply":"2022-10-18T06:45:54.586519Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df.head(10)","metadata":{"execution":{"iopub.status.busy":"2022-10-18T06:45:54.589251Z","iopub.execute_input":"2022-10-18T06:45:54.590141Z","iopub.status.idle":"2022-10-18T06:45:54.640314Z","shell.execute_reply.started":"2022-10-18T06:45:54.590085Z","shell.execute_reply":"2022-10-18T06:45:54.639109Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df.shape","metadata":{"execution":{"iopub.status.busy":"2022-10-18T06:45:54.642127Z","iopub.execute_input":"2022-10-18T06:45:54.643338Z","iopub.status.idle":"2022-10-18T06:45:54.651027Z","shell.execute_reply.started":"2022-10-18T06:45:54.643288Z","shell.execute_reply":"2022-10-18T06:45:54.650115Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df.info()","metadata":{"execution":{"iopub.status.busy":"2022-10-18T06:45:54.655129Z","iopub.execute_input":"2022-10-18T06:45:54.655897Z","iopub.status.idle":"2022-10-18T06:45:54.692762Z","shell.execute_reply.started":"2022-10-18T06:45:54.655851Z","shell.execute_reply":"2022-10-18T06:45:54.691411Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Missing Values\nBefore we go on to build the model, we must look for missing values within the dataset as treating the missing values  is a necessary step before we fit a logistic regression model on the dataset.","metadata":{}},{"cell_type":"code","source":"pd.isnull(df).sum()  # or df.null().sum()","metadata":{"execution":{"iopub.status.busy":"2022-10-18T06:45:54.694717Z","iopub.execute_input":"2022-10-18T06:45:54.695569Z","iopub.status.idle":"2022-10-18T06:45:54.711935Z","shell.execute_reply.started":"2022-10-18T06:45:54.695522Z","shell.execute_reply":"2022-10-18T06:45:54.710471Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The result of this function shows that there are quite a few missing values in columns gender, dependents, city, days since last transaction and Percentage change in credits. Let us go through each of them 1 by 1 to find the appropriate missing value imputation strategy for each of them.","metadata":{}},{"cell_type":"markdown","source":"#### Gender\n\nFor a quick recall let us look at the categories within gender column","metadata":{}},{"cell_type":"code","source":"df['gender'].value_counts()","metadata":{"execution":{"iopub.status.busy":"2022-10-18T06:45:54.714172Z","iopub.execute_input":"2022-10-18T06:45:54.714721Z","iopub.status.idle":"2022-10-18T06:45:54.728722Z","shell.execute_reply.started":"2022-10-18T06:45:54.714671Z","shell.execute_reply":"2022-10-18T06:45:54.727507Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"So there is a good mix of males and females and arguably missing values cannot be filled with any one of them. We could create a seperate category by assigning the value -1 for all missing values in this column.\n\nBefore that, first we will convert the gender into 0/1 and then replace missing values with -1","metadata":{}},{"cell_type":"code","source":"#Convert Gender to encoding values\ndict_gender = {'Male': 1, 'Female':0}\ndf.replace({'gender': dict_gender}, inplace = True)\n\ndf['gender'] = df['gender'].fillna(-1)","metadata":{"execution":{"iopub.status.busy":"2022-10-18T06:45:54.730332Z","iopub.execute_input":"2022-10-18T06:45:54.731416Z","iopub.status.idle":"2022-10-18T06:45:54.752351Z","shell.execute_reply.started":"2022-10-18T06:45:54.731371Z","shell.execute_reply":"2022-10-18T06:45:54.751324Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# check values in Gender column\ndf['gender']","metadata":{"execution":{"iopub.status.busy":"2022-10-18T06:45:54.754055Z","iopub.execute_input":"2022-10-18T06:45:54.755057Z","iopub.status.idle":"2022-10-18T06:45:54.766326Z","shell.execute_reply.started":"2022-10-18T06:45:54.75501Z","shell.execute_reply":"2022-10-18T06:45:54.765261Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Dependents, occupation and city replacing with mode values\n\nNext we will have a quick look at the dependents & occupations column and impute with mode as this is sort of an ordinal variable","metadata":{}},{"cell_type":"code","source":"df['dependents'].value_counts()","metadata":{"execution":{"iopub.status.busy":"2022-10-18T06:45:54.768089Z","iopub.execute_input":"2022-10-18T06:45:54.768882Z","iopub.status.idle":"2022-10-18T06:45:54.785953Z","shell.execute_reply.started":"2022-10-18T06:45:54.768835Z","shell.execute_reply":"2022-10-18T06:45:54.784713Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df['occupation'].value_counts()","metadata":{"execution":{"iopub.status.busy":"2022-10-18T06:45:54.789593Z","iopub.execute_input":"2022-10-18T06:45:54.789963Z","iopub.status.idle":"2022-10-18T06:45:54.803356Z","shell.execute_reply.started":"2022-10-18T06:45:54.789932Z","shell.execute_reply":"2022-10-18T06:45:54.802505Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df['city'].value_counts()","metadata":{"execution":{"iopub.status.busy":"2022-10-18T06:45:54.805072Z","iopub.execute_input":"2022-10-18T06:45:54.80583Z","iopub.status.idle":"2022-10-18T06:45:54.819241Z","shell.execute_reply.started":"2022-10-18T06:45:54.805786Z","shell.execute_reply":"2022-10-18T06:45:54.818091Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df['dependents'] = df['dependents'].fillna(0)         # 0 indicates the index of the 0.0 category variable\ndf['occupation'] = df['occupation'].fillna('self_employed')","metadata":{"execution":{"iopub.status.busy":"2022-10-18T06:45:54.822354Z","iopub.execute_input":"2022-10-18T06:45:54.823113Z","iopub.status.idle":"2022-10-18T06:45:54.833591Z","shell.execute_reply.started":"2022-10-18T06:45:54.823066Z","shell.execute_reply":"2022-10-18T06:45:54.832698Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df['dependents']","metadata":{"execution":{"iopub.status.busy":"2022-10-18T06:45:54.839171Z","iopub.execute_input":"2022-10-18T06:45:54.839819Z","iopub.status.idle":"2022-10-18T06:45:54.853817Z","shell.execute_reply.started":"2022-10-18T06:45:54.839783Z","shell.execute_reply":"2022-10-18T06:45:54.851366Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df['occupation']","metadata":{"execution":{"iopub.status.busy":"2022-10-18T06:45:54.857036Z","iopub.execute_input":"2022-10-18T06:45:54.858097Z","iopub.status.idle":"2022-10-18T06:45:54.874159Z","shell.execute_reply.started":"2022-10-18T06:45:54.85803Z","shell.execute_reply":"2022-10-18T06:45:54.873031Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Similarly City can also be imputed with most common category 1020","metadata":{}},{"cell_type":"code","source":"df['city'] = df['city'].fillna(1020)","metadata":{"execution":{"iopub.status.busy":"2022-10-18T06:45:54.875767Z","iopub.execute_input":"2022-10-18T06:45:54.876676Z","iopub.status.idle":"2022-10-18T06:45:54.886763Z","shell.execute_reply.started":"2022-10-18T06:45:54.876639Z","shell.execute_reply":"2022-10-18T06:45:54.883491Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df['city']","metadata":{"execution":{"iopub.status.busy":"2022-10-18T06:45:54.888984Z","iopub.execute_input":"2022-10-18T06:45:54.890389Z","iopub.status.idle":"2022-10-18T06:45:54.903741Z","shell.execute_reply.started":"2022-10-18T06:45:54.890336Z","shell.execute_reply":"2022-10-18T06:45:54.902132Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Days since Last Transaction\nA fair assumption can be made on this column as this is number of days since last transaction in 1 year, we can substitute missing values with a value greater than 1 year say 999","metadata":{}},{"cell_type":"code","source":"df['days_since_last_transaction'] = df['days_since_last_transaction'].fillna(999)","metadata":{"execution":{"iopub.status.busy":"2022-10-18T06:45:54.90586Z","iopub.execute_input":"2022-10-18T06:45:54.907261Z","iopub.status.idle":"2022-10-18T06:45:54.915777Z","shell.execute_reply.started":"2022-10-18T06:45:54.907209Z","shell.execute_reply":"2022-10-18T06:45:54.914118Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df['days_since_last_transaction']","metadata":{"execution":{"iopub.status.busy":"2022-10-18T06:45:54.917727Z","iopub.execute_input":"2022-10-18T06:45:54.91869Z","iopub.status.idle":"2022-10-18T06:45:54.932022Z","shell.execute_reply.started":"2022-10-18T06:45:54.91864Z","shell.execute_reply":"2022-10-18T06:45:54.930331Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Preprocessing\n\nNow, before applying linear model such as logistic regression, we need to scale the data and keep all features as numeric strictly. \n","metadata":{}},{"cell_type":"markdown","source":"### Dummies with Multiple Categories","metadata":{}},{"cell_type":"code","source":"# Convert occupation to one hot encoded features\ndf = pd.concat([df,pd.get_dummies(df['occupation'],prefix = str('occupation'),prefix_sep='_')],axis = 1)","metadata":{"execution":{"iopub.status.busy":"2022-10-18T06:45:54.934626Z","iopub.execute_input":"2022-10-18T06:45:54.935539Z","iopub.status.idle":"2022-10-18T06:45:54.955588Z","shell.execute_reply.started":"2022-10-18T06:45:54.935487Z","shell.execute_reply":"2022-10-18T06:45:54.954237Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df.head()","metadata":{"scrolled":true,"execution":{"iopub.status.busy":"2022-10-18T06:45:54.957792Z","iopub.execute_input":"2022-10-18T06:45:54.958681Z","iopub.status.idle":"2022-10-18T06:45:55.000447Z","shell.execute_reply.started":"2022-10-18T06:45:54.958629Z","shell.execute_reply":"2022-10-18T06:45:54.999224Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df.shape","metadata":{"execution":{"iopub.status.busy":"2022-10-18T06:45:55.002453Z","iopub.execute_input":"2022-10-18T06:45:55.00345Z","iopub.status.idle":"2022-10-18T06:45:55.012415Z","shell.execute_reply.started":"2022-10-18T06:45:55.003378Z","shell.execute_reply":"2022-10-18T06:45:55.010853Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# copy the orginal data(inculding the encoded variables) to a new dataframe. But the use the old dataframe name.\ndf_df_og = df.copy()","metadata":{"execution":{"iopub.status.busy":"2022-10-18T06:45:55.01493Z","iopub.execute_input":"2022-10-18T06:45:55.016076Z","iopub.status.idle":"2022-10-18T06:45:55.036302Z","shell.execute_reply.started":"2022-10-18T06:45:55.015986Z","shell.execute_reply":"2022-10-18T06:45:55.035014Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Using on-hot encoding we converted 'occupation' feature into encoding form. So that no.of columns increases from 21 to 26.","metadata":{}},{"cell_type":"markdown","source":"### Scaling Numerical Features for Logistic Regression\n\nNow, we remember that there are a lot of outliers in the dataset especially when it comes to previous and current balance features. Also, the distributions are skewed for these features if you recall from the EDA. We will take 2 steps to deal with that here:\n* Log Transformation\n* Standard Scaler\n\nStandard scaling is anyways a necessity when it comes to linear models and we have done that here after doing log transformation on all balance features.","metadata":{}},{"cell_type":"code","source":"num_cols = ['customer_nw_category', 'current_balance',\n            'previous_month_end_balance', 'average_monthly_balance_prevQ2', 'average_monthly_balance_prevQ',\n            'current_month_credit','previous_month_credit', 'current_month_debit', \n            'previous_month_debit','current_month_balance', 'previous_month_balance']\n\n# create a function for the num_cols dataset and do **log transformation**\n# (For all the balance features, we will first add a constant value so that everything becomes postive and then we perform Log tranformation.)\nfor i in num_cols:\n    df[i] = np.log(df[i] + 17000)\n\nstd = StandardScaler()             # create the standard scalar model\nscaled = std.fit_transform(df[num_cols])     # fit the log dataset to the standard scaler\nscaled = pd.DataFrame(scaled,columns=num_cols)  # transform the datset to a pandas dataframe","metadata":{"execution":{"iopub.status.busy":"2022-10-18T06:45:55.038082Z","iopub.execute_input":"2022-10-18T06:45:55.038661Z","iopub.status.idle":"2022-10-18T06:45:55.073559Z","shell.execute_reply.started":"2022-10-18T06:45:55.038536Z","shell.execute_reply":"2022-10-18T06:45:55.07254Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"scaled       # The above taken columns are scaled and printed","metadata":{"execution":{"iopub.status.busy":"2022-10-18T06:45:55.075062Z","iopub.execute_input":"2022-10-18T06:45:55.076175Z","iopub.status.idle":"2022-10-18T06:45:55.100943Z","shell.execute_reply.started":"2022-10-18T06:45:55.076116Z","shell.execute_reply":"2022-10-18T06:45:55.099512Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df = df.drop(columns = num_cols,axis = 1)   # Dropping the 'num_cols' variables from 'df' dataset\ndf = df.merge(scaled,left_index=True,right_index=True,how = \"left\")   # Merge the 'df' dataset and 'scaled' dataset","metadata":{"execution":{"iopub.status.busy":"2022-10-18T06:45:55.103003Z","iopub.execute_input":"2022-10-18T06:45:55.103488Z","iopub.status.idle":"2022-10-18T06:45:55.122922Z","shell.execute_reply.started":"2022-10-18T06:45:55.103443Z","shell.execute_reply":"2022-10-18T06:45:55.121173Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df.columns","metadata":{"execution":{"iopub.status.busy":"2022-10-18T06:45:55.124949Z","iopub.execute_input":"2022-10-18T06:45:55.125432Z","iopub.status.idle":"2022-10-18T06:45:55.135484Z","shell.execute_reply.started":"2022-10-18T06:45:55.125374Z","shell.execute_reply":"2022-10-18T06:45:55.133952Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df.shape","metadata":{"execution":{"iopub.status.busy":"2022-10-18T06:45:55.137041Z","iopub.execute_input":"2022-10-18T06:45:55.137753Z","iopub.status.idle":"2022-10-18T06:45:55.148821Z","shell.execute_reply.started":"2022-10-18T06:45:55.137709Z","shell.execute_reply":"2022-10-18T06:45:55.14777Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Seperating the data and target variables for Model building ","metadata":{}},{"cell_type":"code","source":"y_all = df.churn    # Target variable     \ndf = df.drop(['churn','customer_id','occupation'],axis = 1)   # Data Variables(Removing the churn, customer_id, occupation from the dataset)","metadata":{"execution":{"iopub.status.busy":"2022-10-18T06:45:55.15007Z","iopub.execute_input":"2022-10-18T06:45:55.150588Z","iopub.status.idle":"2022-10-18T06:45:55.169523Z","shell.execute_reply.started":"2022-10-18T06:45:55.150544Z","shell.execute_reply":"2022-10-18T06:45:55.168504Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"y_all      # print the target variable it has 1 column and 28382 rows","metadata":{"execution":{"iopub.status.busy":"2022-10-18T06:45:55.171101Z","iopub.execute_input":"2022-10-18T06:45:55.171796Z","iopub.status.idle":"2022-10-18T06:45:55.180773Z","shell.execute_reply.started":"2022-10-18T06:45:55.17176Z","shell.execute_reply":"2022-10-18T06:45:55.179542Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df    # print the data variables it has 28382 rows and 23 columns","metadata":{"execution":{"iopub.status.busy":"2022-10-18T06:45:55.182718Z","iopub.execute_input":"2022-10-18T06:45:55.183781Z","iopub.status.idle":"2022-10-18T06:45:55.22418Z","shell.execute_reply.started":"2022-10-18T06:45:55.183738Z","shell.execute_reply":"2022-10-18T06:45:55.222827Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Model Building and Evaluation Metrics\nSince this is a binary classification problem, we could use the following 2 popular metrics:\n\n1. Recall\n2. Area under the Receiver operating characteristic curve (AUC-ROC)\n\nNow, we are looking at the recall value here because a customer falsely marked as churn would not be as bad as a customer who was not detected as a churning customer and appropriate measures were not taken by the bank to stop him/her from churning\n\nThe ROC AUC is the area under the curve when plotting the (normalized) true positive rate (x-axis) and the false positive rate (y-axis).\n\nOur main metric here would be Recall values, while AUC ROC Score would take care of how well predicted probabilites are able to differentiate between the 2 classes.","metadata":{}},{"cell_type":"markdown","source":"### Conclusions from EDA\n* For debit values, we see that there is a significant difference in the distribution for churn and non churn and it might be turn out to be an important feature\n* For all the balance features the lower values have much higher proportion of churning customers\n* For most frequent vintage values, the churning customers are slightly higher, while for higher values of vintage, we have mostly non churning customers which is in sync with the age variable \n* We see significant difference for different occupations and certainly would be interesting to use as a feature for prediction of churn.\n\nNow, we will first split our dataset into test and train and using the above conclusions select columns and build a baseline logistic regression model to check the ROC-AUC Score & the confusion matrix","metadata":{}},{"cell_type":"markdown","source":"### Baseline Columns","metadata":{}},{"cell_type":"code","source":"baseline_cols = ['current_month_debit', 'previous_month_debit','current_balance','previous_month_end_balance','vintage'\n                 ,'occupation_retired', 'occupation_salaried','occupation_self_employed', 'occupation_student']","metadata":{"execution":{"iopub.status.busy":"2022-10-18T06:45:55.225513Z","iopub.execute_input":"2022-10-18T06:45:55.225855Z","iopub.status.idle":"2022-10-18T06:45:55.231764Z","shell.execute_reply.started":"2022-10-18T06:45:55.225824Z","shell.execute_reply":"2022-10-18T06:45:55.230492Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_baseline = df[baseline_cols]  # create a dataframe for the above columns. So that we are using those columns as data labels.","metadata":{"execution":{"iopub.status.busy":"2022-10-18T06:45:55.233522Z","iopub.execute_input":"2022-10-18T06:45:55.233943Z","iopub.status.idle":"2022-10-18T06:45:55.245524Z","shell.execute_reply.started":"2022-10-18T06:45:55.233909Z","shell.execute_reply":"2022-10-18T06:45:55.244258Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Train Test Split to create a validation set","metadata":{}},{"cell_type":"code","source":"# Splitting the data into Train and Validation set\nxtrain, xtest, ytrain, ytest = train_test_split(df_baseline,y_all,test_size=1/3, random_state=11, stratify = y_all)","metadata":{"execution":{"iopub.status.busy":"2022-10-18T06:45:55.247024Z","iopub.execute_input":"2022-10-18T06:45:55.247762Z","iopub.status.idle":"2022-10-18T06:45:55.277276Z","shell.execute_reply.started":"2022-10-18T06:45:55.247716Z","shell.execute_reply":"2022-10-18T06:45:55.275921Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(df_baseline.shape, xtrain.shape, xtest.shape)","metadata":{"execution":{"iopub.status.busy":"2022-10-18T06:45:55.279281Z","iopub.execute_input":"2022-10-18T06:45:55.280274Z","iopub.status.idle":"2022-10-18T06:45:55.286481Z","shell.execute_reply.started":"2022-10-18T06:45:55.280223Z","shell.execute_reply":"2022-10-18T06:45:55.285512Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"'baseline' dataset has 28382 rows and 9 columns.\nSince the train and test data are splitted train data takes 70% and test data takes 30%. So xtrain has 18921 rows and 9 columns and xtest has 9461 rows and 9 columns.","metadata":{}},{"cell_type":"code","source":"model = LogisticRegression()    # create a logreg model\nmodel.fit(xtrain,ytrain)        # Fit the xtrain and ytrain data to the logreg model\npred = model.predict_proba(xtest)[:,1]    # predict_proba function gives the probabilites of the model.","metadata":{"execution":{"iopub.status.busy":"2022-10-18T06:45:55.288042Z","iopub.execute_input":"2022-10-18T06:45:55.289085Z","iopub.status.idle":"2022-10-18T06:45:55.973274Z","shell.execute_reply.started":"2022-10-18T06:45:55.288997Z","shell.execute_reply":"2022-10-18T06:45:55.971537Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### AUC ROC Curve & Confusion Matrix \n\nNow, let us quickly look at the AUC-ROC curve for our logistic regression model and also the confusion matrix to see where the logistic regression model is failing here.","metadata":{}},{"cell_type":"code","source":"from sklearn.metrics import roc_curve\nfpr, tpr, _ = roc_curve(ytest,pred) \nauc = roc_auc_score(ytest, pred) \nplt.figure(figsize=(12,8)) \nplt.plot(fpr,tpr,label=\"Validation AUC-ROC=\"+str(auc)) \nx = np.linspace(0, 1, 1000)\nplt.plot(x, x, linestyle='-')\nplt.xlabel('False Positive Rate') \nplt.ylabel('True Positive Rate') \nplt.legend(loc=4) \nplt.show()","metadata":{"scrolled":true,"execution":{"iopub.status.busy":"2022-10-18T06:45:55.975507Z","iopub.execute_input":"2022-10-18T06:45:55.976867Z","iopub.status.idle":"2022-10-18T06:45:56.360157Z","shell.execute_reply.started":"2022-10-18T06:45:55.976814Z","shell.execute_reply":"2022-10-18T06:45:56.35887Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The orange line is the AUC-ROC curve actually represents the random selection. i.e; in the random selection i got 50% FPR and also 50% TPR or true customers who churn.\n\nOn the other hand, using the model(blue curve) i can take top 20% of the model this will give me more than 60% of the customers who would churn.\nFinally, the model is getting information from the variables and is making right predictions.","metadata":{}},{"cell_type":"code","source":"# Confusion Matrix\npred_val = model.predict(xtest)","metadata":{"execution":{"iopub.status.busy":"2022-10-18T06:45:56.36191Z","iopub.execute_input":"2022-10-18T06:45:56.362673Z","iopub.status.idle":"2022-10-18T06:45:56.377436Z","shell.execute_reply.started":"2022-10-18T06:45:56.362622Z","shell.execute_reply":"2022-10-18T06:45:56.375631Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"label_preds = pred_val\n\ncm = confusion_matrix(ytest,label_preds)\n\n\ndef plot_confusion_matrix(cm, normalized=True, cmap='bone'):\n    plt.figure(figsize=[7, 6])\n    norm_cm = cm\n    if normalized:\n        norm_cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n        sns.heatmap(norm_cm, annot=cm, fmt='g', xticklabels=['Predicted: No','Predicted: Yes'], yticklabels=['Actual: No','Actual: Yes'], cmap=cmap)\n\nplot_confusion_matrix(cm, ['No', 'Yes'])","metadata":{"scrolled":true,"execution":{"iopub.status.busy":"2022-10-18T06:45:56.380313Z","iopub.execute_input":"2022-10-18T06:45:56.380994Z","iopub.status.idle":"2022-10-18T06:45:56.7295Z","shell.execute_reply.started":"2022-10-18T06:45:56.380932Z","shell.execute_reply":"2022-10-18T06:45:56.728271Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"From the confusion matrix, if the model predicted that there was 'no chrun' which happened in 9143 cases, out of that 7593 case predicted correctly.\nSimiliarly, if the model predicted 'yes' in case of 318 cases, out of that 203 cases predicted accurately.\nSo the model is able to  extract the information and perform better than the random model.","metadata":{}},{"cell_type":"code","source":"# Recall Score\nrecall_score(ytest,pred_val)","metadata":{"execution":{"iopub.status.busy":"2022-10-18T06:45:56.731174Z","iopub.execute_input":"2022-10-18T06:45:56.732604Z","iopub.status.idle":"2022-10-18T06:45:56.748065Z","shell.execute_reply.started":"2022-10-18T06:45:56.732543Z","shell.execute_reply":"2022-10-18T06:45:56.746646Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Cross validation\n\n\nCross Validation is one of the most important concepts in any type of data modelling. It simply says, try to leave a sample on which you do not train the model and test the model on this sample before finalizing the model.\n\nWe divide the entire population into k equal samples. Now we train models on k-1 samples and validate on 1 sample. Then, at the second iteration we train the model with a different sample held as validation. \n\nIn k iterations, we have basically built model on each sample and held each of them as validation. This is a way to reduce the selection bias and reduce the variance in prediction power.\n\nSince it builds several models on different subsets of the dataset, we can be more sure of our model performance if we use CV for testing our models.","metadata":{}},{"cell_type":"code","source":"def cv_score(ml_model, rstate = 12, thres = 0.5, cols = df.columns):\n    i = 1\n    cv_scores = []\n    df1 = df.copy()\n    df1 = df[cols]\n    \n    # 5 Fold cross validation stratified on the basis of target\n    kf = StratifiedKFold(n_splits=5,random_state=rstate,shuffle=True)\n    for df_index,test_index in kf.split(df1,y_all):\n        print('\\n{} of kfold {}'.format(i,kf.n_splits))\n        xtr,xvl = df1.loc[df_index],df1.loc[test_index]\n        ytr,yvl = y_all.loc[df_index],y_all.loc[test_index]\n            \n        # Define model for fitting on the training set for each fold\n        model = ml_model\n        model.fit(xtr, ytr)\n        pred_probs = model.predict_proba(xvl)\n        pp = []\n         \n        # Use threshold to define the classes based on probability values\n        for j in pred_probs[:,1]:\n            if j>thres:\n                pp.append(1)\n            else:\n                pp.append(0)\n         \n        # Calculate scores for each fold and print\n        pred_val = pp\n        roc_score = roc_auc_score(yvl,pred_probs[:,1])\n        recall = recall_score(yvl,pred_val)\n        precision = precision_score(yvl,pred_val)\n        sufix = \"\"\n        msg = \"\"\n        msg += \"ROC AUC Score: {}, Recall Score: {:.4f}, Precision Score: {:.4f} \".format(roc_score, recall,precision)\n        print(\"{}\".format(msg))\n         \n         # Save scores\n        cv_scores.append(roc_score)\n        i+=1\n    return cv_scores","metadata":{"execution":{"iopub.status.busy":"2022-10-18T06:45:56.756974Z","iopub.execute_input":"2022-10-18T06:45:56.758487Z","iopub.status.idle":"2022-10-18T06:45:56.781339Z","shell.execute_reply.started":"2022-10-18T06:45:56.758436Z","shell.execute_reply":"2022-10-18T06:45:56.77968Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"baseline_scores = cv_score(LogisticRegression(), cols = baseline_cols)","metadata":{"execution":{"iopub.status.busy":"2022-10-18T06:45:56.783126Z","iopub.execute_input":"2022-10-18T06:45:56.783856Z","iopub.status.idle":"2022-10-18T06:46:00.433233Z","shell.execute_reply.started":"2022-10-18T06:45:56.783812Z","shell.execute_reply":"2022-10-18T06:46:00.431977Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Now let us try using all columns available to check if we get significant improvement.","metadata":{}},{"cell_type":"code","source":"all_feat_scores = cv_score(LogisticRegression())","metadata":{"execution":{"iopub.status.busy":"2022-10-18T06:46:00.434806Z","iopub.execute_input":"2022-10-18T06:46:00.435614Z","iopub.status.idle":"2022-10-18T06:46:03.564974Z","shell.execute_reply.started":"2022-10-18T06:46:00.435568Z","shell.execute_reply":"2022-10-18T06:46:03.560766Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"There is some improvement in both ROC AUC Scores and Precision/Recall Scores. Now we can try backward selection to select the best subset of features which give the best score. ","metadata":{}},{"cell_type":"markdown","source":"### Reverse Feature Elimination or Backward Selection\n\nWe have already built a model using all the features and a separate model using some baseline features. We can try using backward feature elimination to check if we can do better. Let's do that next.","metadata":{}},{"cell_type":"code","source":"from sklearn.feature_selection import RFE\nimport matplotlib.pyplot as plt\n\n# Create the RFE object and rank each feature\nmodel = LogisticRegression()\nrfe = RFE(estimator=model, n_features_to_select=1, step=1)\nrfe.fit(df, y_all)","metadata":{"execution":{"iopub.status.busy":"2022-10-18T06:46:03.566888Z","iopub.execute_input":"2022-10-18T06:46:03.567366Z","iopub.status.idle":"2022-10-18T06:46:11.439248Z","shell.execute_reply.started":"2022-10-18T06:46:03.567319Z","shell.execute_reply":"2022-10-18T06:46:11.437957Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"ranking_df = pd.DataFrame()\nranking_df['Feature_name'] = df.columns\nranking_df['Rank'] = rfe.ranking_","metadata":{"execution":{"iopub.status.busy":"2022-10-18T06:46:11.44134Z","iopub.execute_input":"2022-10-18T06:46:11.44254Z","iopub.status.idle":"2022-10-18T06:46:11.454905Z","shell.execute_reply.started":"2022-10-18T06:46:11.442492Z","shell.execute_reply":"2022-10-18T06:46:11.45253Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"ranked = ranking_df.sort_values(by=['Rank'])","metadata":{"execution":{"iopub.status.busy":"2022-10-18T06:46:11.457851Z","iopub.execute_input":"2022-10-18T06:46:11.459202Z","iopub.status.idle":"2022-10-18T06:46:11.467291Z","shell.execute_reply.started":"2022-10-18T06:46:11.459149Z","shell.execute_reply":"2022-10-18T06:46:11.465363Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"ranked","metadata":{"execution":{"iopub.status.busy":"2022-10-18T06:46:11.470226Z","iopub.execute_input":"2022-10-18T06:46:11.47145Z","iopub.status.idle":"2022-10-18T06:46:11.492704Z","shell.execute_reply.started":"2022-10-18T06:46:11.471382Z","shell.execute_reply":"2022-10-18T06:46:11.491479Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The balance features are proving to be very important as can be seen from the table. The RFE function can also be used to select features. Lets select the top 10 features from this table and check score.","metadata":{}},{"cell_type":"code","source":"rfe_top_10_scores = cv_score(LogisticRegression(), cols = ranked['Feature_name'][:10].values)","metadata":{"execution":{"iopub.status.busy":"2022-10-18T06:46:11.494854Z","iopub.execute_input":"2022-10-18T06:46:11.495717Z","iopub.status.idle":"2022-10-18T06:46:12.845584Z","shell.execute_reply.started":"2022-10-18T06:46:11.495671Z","shell.execute_reply":"2022-10-18T06:46:12.844207Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Wow, the top 10 features obtained using the reverse feature selection are giving a much better score than any of our earlier attempts. This is the power of feature selection and it especially works well in case of linear models as tree based models are in itself to some extent capable of doing feature selection.","metadata":{}},{"cell_type":"markdown","source":"The recall score here is quite low. We should play around with the threshold to get a better recall score. AUC ROC depends on the predicted probabilities and is not impacted by the threshold. Let us try 0.2 as threshold which is close to the overall churn rate","metadata":{}},{"cell_type":"code","source":"cv_score(LogisticRegression(), cols = ranked['Feature_name'][:10].values, thres=0.14)","metadata":{"execution":{"iopub.status.busy":"2022-10-18T06:46:12.848563Z","iopub.execute_input":"2022-10-18T06:46:12.849822Z","iopub.status.idle":"2022-10-18T06:46:14.20791Z","shell.execute_reply.started":"2022-10-18T06:46:12.849765Z","shell.execute_reply":"2022-10-18T06:46:14.206305Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We observe that there is continuous improvement in the Recall Score. However, clearly precision score is going down. On the basis of business requirement the bank can take a call on deciding the threshold. Without knowing the metrics relevant to the business, our best course of action is to optimize for AUC ROC Score so as to find the best probabilites here.","metadata":{}},{"cell_type":"markdown","source":"## Comparison of Different model fold wise\n\nLet us visualise the cross validation scores for each fold for the following 3 models and observe differences:\n* Baseline Model\n* Model based on all features\n* Model based on top 10 features obtained from RFE","metadata":{}},{"cell_type":"code","source":"results_df = pd.DataFrame({'baseline':baseline_scores, 'all_feats': all_feat_scores, 'rfe_top_10': rfe_top_10_scores})","metadata":{"execution":{"iopub.status.busy":"2022-10-18T06:46:14.210092Z","iopub.execute_input":"2022-10-18T06:46:14.211031Z","iopub.status.idle":"2022-10-18T06:46:14.219868Z","shell.execute_reply.started":"2022-10-18T06:46:14.210983Z","shell.execute_reply":"2022-10-18T06:46:14.217844Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"results_df.plot(y=[\"baseline\", \"all_feats\", \"rfe_top_10\"], kind=\"bar\")","metadata":{"execution":{"iopub.status.busy":"2022-10-18T06:46:14.221896Z","iopub.execute_input":"2022-10-18T06:46:14.222369Z","iopub.status.idle":"2022-10-18T06:46:14.521484Z","shell.execute_reply.started":"2022-10-18T06:46:14.222326Z","shell.execute_reply":"2022-10-18T06:46:14.520147Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Here, we can see that the model based on RFE is giving the best result for each fold and students are encouraged to try more feature selection techniques and fine tune to get the best results.","metadata":{}}]}